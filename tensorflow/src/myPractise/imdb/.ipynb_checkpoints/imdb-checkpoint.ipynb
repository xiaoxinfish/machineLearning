{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "NUM_SAMPLE = 10000\n",
    "reverse_word_index = None\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "history = None\n",
    "baseline_history = None\n",
    "smaller_history = None\n",
    "bigger_history = None\n",
    "l2_model_history = None\n",
    "dpt_model_history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and create dictionary for word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDatasets():\n",
    "    global train_data, train_labels, test_data, test_labels\n",
    "    imdb = keras.datasets.imdb\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000, path=os.getcwd() + r\"\\datasets\\imdb.npz\")\n",
    "    print(\"Training entries:{}, labels:{}\".format(len(train_data), len(train_labels)))\n",
    "    print(\"train_data.shape:{}\\nlength of train_data[0]:{} length of train_data[1]:{}\"\n",
    "          .format(train_data.shape, len(train_data[0]), len(train_data[1])))\n",
    "    \n",
    "    global reverse_word_index\n",
    "    # A dictionary mapping words to an integer index\n",
    "    word_index = imdb.get_word_index(path=os.getcwd() + r\"\\datasets\\imdb_word_index.json\")\n",
    "\n",
    "    # The first indices are reserved\n",
    "    word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2  # unknown\n",
    "    word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "    \n",
    "    train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                           value = word_index[\"<PAD>\"],\n",
    "                                                           padding = 'post',\n",
    "                                                           maxlen = 256)\n",
    "    test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_sequences(sequences, dimension):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restractOverFit():\n",
    "    global train_data, test_data\n",
    "    train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
    "    test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS) \n",
    "    plt.plot(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index[i] for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neualNetwork():\n",
    "    global history\n",
    "    vocab_size = 10000\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(16, activation = tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "    model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    x_val = train_data[:10000]\n",
    "    partial_x_train = train_data[10000:]\n",
    "    y_val = train_labels[:10000]\n",
    "    partial_y_train = train_labels[10000:]\n",
    "    \n",
    "    history = model.fit(partial_x_train,\n",
    "                       partial_y_train,\n",
    "                       epochs = 40,\n",
    "                       batch_size =512,\n",
    "                       validation_data = (x_val, y_val),\n",
    "                       verbose =1)\n",
    "    result = model.evaluate(test_data, test_labels)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseNN():\n",
    "    global baseline_history\n",
    "    baseline_model = keras.Sequential([\n",
    "        # `input_shape` is only required here so that `.summary` works.\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    baseline_model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    baseline_model.summary()    \n",
    "\n",
    "    baseline_history = baseline_model.fit(train_data,\n",
    "                                      train_labels,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(test_data, test_labels),\n",
    "                                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallerNN():\n",
    "    global smaller_history\n",
    "    smaller_model = keras.Sequential([\n",
    "        keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    smaller_model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    smaller_model.summary()    \n",
    "    smaller_history = smaller_model.fit(train_data,\n",
    "                                        train_labels,\n",
    "                                        epochs=20,\n",
    "                                        batch_size=512,\n",
    "                                        validation_data=(test_data, test_labels),\n",
    "                                        verbose=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggerNN():\n",
    "    global bigger_history\n",
    "    bigger_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    bigger_model.compile(optimizer='adam',\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy','binary_crossentropy'])\n",
    "\n",
    "    bigger_model.summary()    \n",
    "    bigger_history = bigger_model.fit(train_data, train_labels,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(test_data, test_labels),\n",
    "                                      verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2NN():\n",
    "    global l2_model_history\n",
    "    l2_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    l2_model.compile(optimizer='adam',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy', 'binary_crossentropy'])\n",
    "\n",
    "    l2_model_history = l2_model.fit(train_data, train_labels,\n",
    "                                    epochs=20,\n",
    "                                    batch_size=512,\n",
    "                                    validation_data=(test_data, test_labels),\n",
    "                                    verbose=2)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomDropoutNN():\n",
    "    global dpt_model_history\n",
    "    dpt_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(NUM_WORDS,)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    dpt_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy','binary_crossentropy'])\n",
    "\n",
    "    dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(test_data, test_labels),\n",
    "                                      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories, key='binary_crossentropy'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawTrade():\n",
    "    import matplotlib.pyplot as plt\n",
    "    global history\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()   # clear figure\n",
    "    acc_values = history.history['acc']\n",
    "    val_acc_values = history.history['val_acc']\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries:25000, labels:25000\n",
      "train_data.shape:(25000,)\n",
      "length of train_data[0]:218 length of train_data[1]:189\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 9s - loss: 0.6383 - acc: 0.6200 - binary_crossentropy: 0.6383 - val_loss: 0.5255 - val_acc: 0.8284 - val_binary_crossentropy: 0.5255\n",
      "Epoch 2/20\n",
      " - 6s - loss: 0.4995 - acc: 0.7940 - binary_crossentropy: 0.4995 - val_loss: 0.4005 - val_acc: 0.8741 - val_binary_crossentropy: 0.4005\n",
      "Epoch 3/20\n",
      " - 6s - loss: 0.3959 - acc: 0.8618 - binary_crossentropy: 0.3959 - val_loss: 0.3303 - val_acc: 0.8808 - val_binary_crossentropy: 0.3303\n",
      "Epoch 4/20\n",
      " - 8s - loss: 0.3245 - acc: 0.8921 - binary_crossentropy: 0.3245 - val_loss: 0.3030 - val_acc: 0.8833 - val_binary_crossentropy: 0.3030\n",
      "Epoch 5/20\n",
      " - 5s - loss: 0.2776 - acc: 0.9079 - binary_crossentropy: 0.2776 - val_loss: 0.2960 - val_acc: 0.8819 - val_binary_crossentropy: 0.2960\n",
      "Epoch 6/20\n",
      " - 6s - loss: 0.2407 - acc: 0.9215 - binary_crossentropy: 0.2407 - val_loss: 0.3011 - val_acc: 0.8800 - val_binary_crossentropy: 0.3011\n",
      "Epoch 7/20\n",
      " - 5s - loss: 0.2092 - acc: 0.9320 - binary_crossentropy: 0.2092 - val_loss: 0.3109 - val_acc: 0.8774 - val_binary_crossentropy: 0.3109\n",
      "Epoch 8/20\n",
      " - 6s - loss: 0.1847 - acc: 0.9417 - binary_crossentropy: 0.1847 - val_loss: 0.3382 - val_acc: 0.8767 - val_binary_crossentropy: 0.3382\n",
      "Epoch 9/20\n",
      " - 6s - loss: 0.1635 - acc: 0.9471 - binary_crossentropy: 0.1635 - val_loss: 0.3503 - val_acc: 0.8756 - val_binary_crossentropy: 0.3503\n",
      "Epoch 10/20\n",
      " - 6s - loss: 0.1479 - acc: 0.9508 - binary_crossentropy: 0.1479 - val_loss: 0.3760 - val_acc: 0.8737 - val_binary_crossentropy: 0.3760\n",
      "Epoch 11/20\n",
      " - 6s - loss: 0.1333 - acc: 0.9562 - binary_crossentropy: 0.1333 - val_loss: 0.3998 - val_acc: 0.8729 - val_binary_crossentropy: 0.3998\n",
      "Epoch 12/20\n",
      " - 5s - loss: 0.1258 - acc: 0.9567 - binary_crossentropy: 0.1258 - val_loss: 0.4274 - val_acc: 0.8724 - val_binary_crossentropy: 0.4274\n",
      "Epoch 13/20\n",
      " - 6s - loss: 0.1166 - acc: 0.9602 - binary_crossentropy: 0.1166 - val_loss: 0.4376 - val_acc: 0.8718 - val_binary_crossentropy: 0.4376\n",
      "Epoch 14/20\n",
      " - 7s - loss: 0.1071 - acc: 0.9637 - binary_crossentropy: 0.1071 - val_loss: 0.4632 - val_acc: 0.8699 - val_binary_crossentropy: 0.4632\n",
      "Epoch 15/20\n",
      " - 6s - loss: 0.1004 - acc: 0.9648 - binary_crossentropy: 0.1004 - val_loss: 0.4674 - val_acc: 0.8714 - val_binary_crossentropy: 0.4674\n",
      "Epoch 16/20\n",
      " - 5s - loss: 0.0967 - acc: 0.9659 - binary_crossentropy: 0.0967 - val_loss: 0.4783 - val_acc: 0.8706 - val_binary_crossentropy: 0.4783\n",
      "Epoch 17/20\n",
      " - 5s - loss: 0.0884 - acc: 0.9676 - binary_crossentropy: 0.0884 - val_loss: 0.5198 - val_acc: 0.8696 - val_binary_crossentropy: 0.5198\n",
      "Epoch 18/20\n",
      " - 7s - loss: 0.0849 - acc: 0.9677 - binary_crossentropy: 0.0849 - val_loss: 0.5460 - val_acc: 0.8688 - val_binary_crossentropy: 0.5460\n",
      "Epoch 19/20\n",
      " - 5s - loss: 0.0794 - acc: 0.9701 - binary_crossentropy: 0.0794 - val_loss: 0.5537 - val_acc: 0.8690 - val_binary_crossentropy: 0.5537\n",
      "Epoch 20/20\n",
      " - 6s - loss: 0.0798 - acc: 0.9691 - binary_crossentropy: 0.0798 - val_loss: 0.5787 - val_acc: 0.8701 - val_binary_crossentropy: 0.5787\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-1436c517f32e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     plot_history([('baseline', baseline_history),\n\u001b[0;32m     21\u001b[0m                   \u001b[1;33m(\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_model_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     ('dropout', dpt_model_history)])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-e116e8a5ac38>\u001b[0m in \u001b[0;36mplot_history\u001b[1;34m(histories, key)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         val = plt.plot(history.epoch, history.history['val_'+key],\n\u001b[0m\u001b[0;32m      6\u001b[0m                    '--', label=name.title()+' Val')\n\u001b[0;32m      7\u001b[0m         plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'epoch'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    isPrepareDatasetsNeeded = True\n",
    "    if isPrepareDatasetsNeeded:\n",
    "        loadDatasets()\n",
    "    restractOverFit()\n",
    "    isTrainingNeeded = False\n",
    "    if isTrainingNeeded:\n",
    "        #neualNetwork()\n",
    "        baseNN()\n",
    "        smallerNN()\n",
    "        biggerNN()\n",
    "        L2NN()\n",
    "        randomDropoutNN()        \n",
    "        plot_history([('baseline', baseline_history),\n",
    "                  ('smaller', smaller_history),\n",
    "                  ('bigger', bigger_history),\n",
    "                  ('l2', l2_model_history),\n",
    "                  ('dropout', dpt_model_history)])                      \n",
    "    #print(decode_review(train_data[0]))\n",
    "    #drawTrade()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
